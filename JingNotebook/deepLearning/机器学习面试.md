## 机器学习算法面试六十八问
### 分类问题
1. 交叉熵公式？
交叉熵：设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是：
\[{\rm{D(p||q) = }}\sum\limits_x {p(x)\log \frac{{p(x)}}{{q(x)}}}  = {E_{p(x)}}\log \frac{{p(x)}}{{q(x)}}\]
在一定程度上，相对熵可以度量两个随机变量的“距离”，且有D(p||q) ≠D(q||p)。另外，值得一提的是，D(p||q)是必然大于等于0的。

互信息：两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵，用I(X,Y)表示：
\[{\rm{I(X,Y) = }}\sum\limits_{x,y} {p(x,y)\log \frac{{p(x,y)}}{{p(x)p(y)}}} \]

2.  LR公式？
逻辑回归本质上是线性回归，只是在特征到结果的映射中加入了一层逻辑函数g(z)，即先把特征线性求和，然后使用函数g(z)作为假设函数来预测。g(z)可以将连续值映射到0 和1。g(z)为sigmoid function.
\[\begin{array}{l}
{\rm{g}}(z) = \frac{1}{{1 + {e^{ - z}}}}\\
{h_\theta }(x) = g({\theta ^T}x) = \frac{1}{{1 + {e^{ - {\theta ^T}x}}}}
\end{array}\]
3. 伯努利分布？
\[\begin{array}{l}
{\rm{P}}(y = 1|x;\theta ) = {h_\theta }(x)\\
{\rm{P}}(y = 0|x;\theta ) = 1 - {h_\theta }(x)
\end{array}\]
4. 极大似然函数？
对于训练数据集，特征数据x={x1, x2, … , xm}和对应的分类标签y={y1, y2, … , ym}，假设m个样本是相互独立的，那么，极大似然函数为：
\[\begin{array}{l}
L(\theta ) = p(\vec y|X;\theta )\\
 = \prod\limits_{i = 1}^m {p({y^{(i)}}|{x^{(i)}};\theta )} 
\end{array}\]

5. 逻辑回归怎么实现多分类？
方式一:修改逻辑回归的损失函数,使用softmax函数构造模型解决多分类问题,softmax分类模型会有相同于类别数的输出,输出的值为对于样本属于各个类别的概率,最后对于样本进行预测的类型为概率值最高的那个类别。
方式二:根据每个类别都建立一个二分类器,本类别的样本标签定义为0,其它分类样本标签定义为1,则有多少个类别就构造多少个逻辑回归分类器
若所有类别之间有明显的互斥则使用softmax分类器,若所有类别不互斥有交叉的情况则构造相应类别个数的逻辑回归分类器。
6. SVM中什么时候用线性核什么时候用高斯核?
当数据的特征提取的较好,所包含的信息量足够大,很多问题是线性可分的那么可以采用线性核。若特征数较少,样本数适中,对于时间不敏感,遇到的问题是线性不可分的时候可以使用高斯核来达到更好的效果。
7. 什么是支持向量机,SVM与LR的区别?
支持向量机为一个二分类模型,它的基本模型定义为特征空间上的间隔最大的线性分类器。而它的学习策略为最大化分类间隔,最终可转化为凸二次规划问题求解。
LR是参数模型,SVM为非参数模型。LR采用的损失函数为logisticalloss,而SVM采用的是hingeloss。在学习分类器的时候,SVM只考虑与分类最相关的少数支持向量点。LR的模型相对简单,在进行大规模线性分类时比较方便。
8. 监督学习和无监督学习的区别？
输入的数据有标签则为监督学习,输入数据无标签为非监督学习。
9. 机器学习中的距离计算方法?
设空间中两个点为$(x_1, y_1)、(x_2, y_2)$
- 欧式距离：
\[\sqrt {{{{\rm{(}}{{\rm{x}}_1}{\rm{ - }}{{\rm{x}}_2}{\rm{)}}}^2}{\rm{ + (}}{{\rm{y}}_1}{\rm{ - }}{{\rm{y}}_2}{{\rm{)}}^2}} \]
- 曼哈顿距离：
\[{{\rm{x}}_1} - {x_2}\left| {{y_1} - {y_2}} \right|\]
- 余弦距离：
\[\cos  = \frac{{{x_1}*{x_2} + {y_1}*{y_2}}}{{\sqrt {x_1^2 + y_1^2} *\sqrt {x_2^2 + y_2^2} }}\]

- 契比雪夫距离：
\[{x_1} - {x_2}{y_1} - {y_2}\]

10. 朴素贝叶斯（naive Bayes）法的要求是？
贝叶斯定理、特征条件独立假设
解析：朴素贝叶斯属于生成式模型，学习输入和输出的联合概率分布。给定输入x，利用贝叶斯概率定理求出最大的后验概率作为输出y。

11. 训练集中类别不均衡，哪个参数最不准确？
准确度（Accuracy）
解析：举例，对于二分类问题来说，正负样例比相差较大为99:1，模型更容易被训练成预测较大占比的类别。因为模型只需要对每个样例按照0.99的概率预测正类，该模型就能达到99%的准确率。

12. SVM的作用，基本实现原理；
SVM可以用于解决二分类或者多分类问题，此处以二分类为例。SVM的目标是寻找一个最优化超平面在空间中分割两类数据，这个最优化超平面需要满足的条件是：离其最近的点到其的距离最大化，这些点被称为支持向量。
解析：建议练习推导SVM，从基本式的推导，到拉格朗日对偶问题。
13. SVM的硬间隔，软间隔表达式；
- 硬间隔：
\[\begin{array}{l}
\mathop {\min }\limits_{w,b} \frac{1}{2}{\left\| w \right\|^2}\\
s.t:{y^{(i)}}({w^T}{x^{(i)}} + b) >  = 1
\end{array}\]
- 软间隔：
\[\begin{array}{l}
\mathop {\min }\limits_{w,b} \frac{1}{2}{\left\| w \right\|^2} + C\sum\limits_{i = 1}^m {{\xi _i}} \\
s.t:{y^{(i)}}({w^T}{x^{(i)}} + b) >  = 1 - {\xi _i};{\xi _i} >  = 0
\end{array}\]

解析：不同点在于有无引入松弛变量

14. SVM使用对偶计算的目的是什么，如何推出来的，手写推导？
目的有两个：一是方便核函数的引入；二是原问题的求解复杂度与特征的维数相关，而转成对偶问题后只与问题的变量个数有关。由于SVM的变量个数为支持向量的个数，相较于特征位数较少，因此转对偶问题。通过拉格朗日算子发使带约束的优化目标转为不带约束的优化函数，使得W和b的偏导数等于零，带入原来的式子，再通过转成对偶问题。



15. SVM的物理意义是什么？
构造一个最优化的超平面在空间中分割数据

16. 如果给你一些数据集，你会如何分类（我是分情况答的，从数据的大小，特征，是否有缺失，分情况分别答的）；
根据数据类型选择不同的模型，如Lr或者SVM，决策树。假如特征维数较多，可以选择SVM模型，如果样本数量较大可以选择LR模型，但是LR模型需要进行数据预处理；假如缺失值较多可以选择决策树。选定完模型后，相应的目标函数就确定了。还可以在考虑正负样例比比，通过上下集采样平衡正负样例比。
解析：需要了解多种分类模型的优缺点，以及如何构造分类模型的步骤

17. 如果数据有问题，怎么处理？
1.上下采样平衡正负样例比；
2.考虑缺失值；
3.数据归一化

18. 分层抽样的适用范围？
分层抽样利用事先掌握的信息,充分考虑了保持样本结构和总体结构的一致性,当总体由差异明显的几部分组成的时候,适合用分层抽样。

19. LR的损失函数？
\[J(\theta ) =  - \frac{1}{M}\sum\nolimits_{i = 1}^M {[{y_i}\log ({h_\theta }({x_i})) + (1 - {y_i})\log (1 - {h_\theta }({x_i}))]} \]
M为样本个数,${{h_\theta }({x_i})}$为模型对样本i的预测结果,$y_i$为样本i的真实标签

20. LR和线性回归的区别?
线性回归用来做预测,LR用来做分类。线性回归是来拟合函数,LR是来预测函数。线性回归用最小二乘法来计算参数,LR用最大似然估计来计算参数。线性回归更容易受到异常值的影响,而LR对异常值有较好的稳定性。

21. 生成模型和判别模型基本形式，有哪些？
生成式：朴素贝叶斯、HMM、Gaussians、马尔科夫随机场
判别式：LR，SVM，神经网络，CRF，Boosting
详情：支持向量机

22. 核函数的种类和应用场景?
线性核、多项式核、高斯核。
特征维数高选择线性核
样本数量可观、特征少选择高斯核（非线性核）
样本数量非常多选择线性核（避免造成庞大的计算量）
详情：支持向量机

23. 分类算法列一下有多少种？应用场景
单一的分类方法主要包括：LR逻辑回归，SVM支持向量机，DT决策树、NB朴素贝叶斯、NN人工神经网络、K-近邻；集成学习算法：基于Bagging和Boosting算法思想，RF随机森林,GBDT，Adaboost,XGboost。

24. SVM核函数的选择?
当样本的特征很多且维数很高时可考虑用SVM的线性核函数。当样本的数量较多,特征较少时,一般手动进行特征的组合再使用SVM的线性核函数。当样本维度不高且数量较少时,且不知道该用什么核函数时一般优先使用高斯核函数,因为高斯核函数为一种局部性较强的核函数,无论对于大样本还是小样本均有较好的性能且相对于多项式核函数有较少的参数。

25. SVM的损失函数:
\[J(\theta ) = \frac{1}{2}{\left\| \theta  \right\|^2} + C\sum\limits_i {\max (0,1 - {y_i}({\theta ^T}{x_i} + b))} \]

26. 核函数的作用:
核函数隐含着一个从低维空间到高维空间的映射,这个映射可以把低维空间中线性不可分的两类点变成线性可分的。

27. SVM为什么使用对偶函数求解?
对偶将原始问题中的约束转为了对偶问题中的等式约束,而且更加方便了核函数的引入,同时也改变了问题的复杂度,在原始问题下,求解问题的复杂度只与样本的维度有关,在对偶问题下,只与样本的数量有关。

28.  ID3,C4.5和CART三种决策树的区别?
ID3决策树优先选择信息增益大的属性来对样本进行划分,但是这样的分裂节点方法有一个很大的缺点,当一个属性可取值数目较多时,可能在这个属性对应值下的样本只有一个或者很少个,此时它的信息增益将很高,ID3会认为这个属性很适合划分,但实际情况下叫多属性的取值会使模型的泛化能力较差,所以C4.5不采用信息增益作为划分依据,而是采用信息增益率作为划分依据。但是仍不能完全解决以上问题,而是有所改善,这个时候引入了CART树,它使用gini系数作为节点的分裂依据。

29. SVM和全部数据有关还是和局部数据有关?
SVM只和分类界限上的支持向量点有关,换而言之只和局部数据有关。

30. 为什么高斯核能够拟合无穷维度?
因为将泰勒展开式代入高斯核,将会得到一个无穷维度的映射。

31. 完整推导svm一遍，还有强化学习说一说，dqn的各种trick了解多少，以及都怎么实现

32. LR和SVM 区别?
1）LR是参数模型，SVM是非参数模型。2）从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。3）SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。4）逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。5）logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

33. 朴素贝叶斯基本原理和预测过程?
朴素贝叶斯分类和预测算法的原理
决策树和朴素贝叶斯是最常用的两种分类算法，本篇文章介绍朴素贝叶斯算法。贝叶斯定理是以英国数学家贝叶斯命名，用来解决两个条件概率之间的关系问题。简单的说就是在已知P(A|B)时如何获得P(B|A)的概率。朴素贝叶斯（Naive Bayes）假设特征P(A)在特定结果P(B)下是独立的。

1.概率基础：

在开始介绍贝叶斯之前，先简单介绍下概率的基础知识。概率是某一结果出现的可能性。例如，抛一枚匀质硬币，正面向上的可能性多大？概率值是一个0-1之间的数字，用来衡量一个事件发生可能性的大小。概率值越接近1，事件发生的可能性越大，概率值越接近0，事件越不可能发生。我们日常生活中听到最多的是天气预报中的降水概率。概率的表示方法叫维恩图。下面我们通过维恩图来说明贝叶斯公式中常见的几个概率。
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/weien.png" width = "380" height = "280" /></div>

在维恩图中：
S：S是样本空间，是所有可能事件的总和。
P(A)：是样本空间S中A事件发生的概率，维恩图中绿色的部分。
P(B)：是样本空间S中B事件发生的概率，维恩图中蓝色的部分。
P(A∩B)：是样本空间S中A事件和B事件同时发生的概率，也就是A和B相交的区域。
P(A|B)：是条件概率，是B事件已经发生时A事件发生的概率。

对于条件概率，还有一种更清晰的表示方式叫概率树。下面的概率树表示了条件概率P(A|B)。与维恩图中的P(A∩B)相比，可以发现两者明显的区别。P(A∩B)是事件A和事件B同时发现的情况，因此是两者相交区域的概率。而事件概率P(A|B)是事件B发生时事件A发生的概率。这里有一个先决条件就是P(B)要首先发生。
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/gailvshu_P(AB).png" width = "380" height = "180" /></div>
因为条件概率P(A|B)是在事件B已经发生的情况下，事件A发生的概率，因此P(A|B)可以表示为事件A与B的交集与事件B的比率。

\[P(A|B) = \frac{{P(A \cap B)}}{{P(B)}}\]
\[P(A \cap B) = P(A|B)P(B)\]

2.贝叶斯公式：
贝叶斯算法通过已知的P(A|B)，P(A),和P(B)三个概率计算P(B|A)发生的概率。假设我们现在已知P(A|B)，P(A)和P(B)三个概率，如何计算P(B|A)呢？通过前面的概率树及P(A|B)的概率可知，P(B|A)的概率是在事件A发生的前提下事件B发生的概率，因此P(B|A)可以表示为事件B与事件A的交集与事件A的比率。
\[\begin{array}{l}
P({\rm{B}}|A) = \frac{{P(B \cap A)}}{{P(A)}}\\
P(B \cap A) = P(B|A)P(A)
\end{array}\]
到这一步，我们只需要证明P(A∩B)= P(B∩A)就可以证明在已知P(A|B)的情况下可以通过计算获得P(B|A)的概率，将概率树转化为下面的概率表，分别列出P(A|B),P(B|A),P(A),和P(B)的概率：
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/beys2.png" width = "380" height = "280" /></div>
通过计算可以证明P(A|B)*P(B)和P(B|A)*P(A)最后求得的结果是概率表中的同一个区域的值，因此：

\[P(A \cap B) = P(B \cap A)\]

我们通过$P(A∩B)= P(B∩A)$证明了在已知P(A|B)，P(A),和P(B)三个概率的情况下可以计算出P(B|A)发生的概率。整个推导和计算过程可以说得通。但从统计学的角度来看，P(A|B)和P(B|A)两个条件概率之间存在怎样的关系呢？我们从贝叶斯推断里可以找到答案。
3.贝叶斯推断：
贝叶斯推断可以说明贝叶斯定理中两个条件概率之间的关系。换句话说就是我们为什么可以通过P(A|B)，P(A),和P(B)三个概率计算出P(B|A)发生的概率。

\[P(B|A) = \frac{{P(A|B)*P(B)}}{{P(A)}}\]

在贝叶斯推断中，每一种概率都有一个特定的名字：
 - P(B)是”先验概率”(Prior probability)。
* P(A)是”先验概率”(Prior probability)，也作标准化常量(normalized constant)。
- P(A|B)是已知B发生后A的条件概率，叫做似然函数(likelihood)。
- P(B|A)是已知A发生后B的条件概率，是我们要求的值，叫做后验概率。
- P(A|B)/P(A)是调整因子，也被称作标准似然度（standardised likelihood）。

贝叶斯推断中有几个关键的概念需要说明下：
- 第一个是先验概率，先验概率是指我们主观通过事件发生次数对概率的判断。
- 第二个是似然函数，似然函数是对某件事发生可能性的判断，与条件概率正好相反。通过事件已经发生的概率推算事件可能性的概率。
维基百科中对似然函数与概率的解释：
概率：是给定某一参数值，求某一结果的可能性。
例如，抛一枚匀质硬币，抛10次，6次正面向上的可能性多大？
似然函数：给定某一结果，求某一参数值的可能性。
例如，抛一枚硬币，抛10次，结果是6次正面向上，其是匀质的可能性多大？

- 第三个是调整因子：调整因子是似然函数与先验概率的比值，这个比值相当于一个权重，用来调整后验概率的值，使后验概率更接近真实概率。调整因子有三种情况，大于1，等于1和小于1。
调整因子P(A|B)/P(A)>1：说明事件可能发生的概率要大于事件已经发生次数的概率。
调整因子P(A|B)/P(A)=1：说明事件可能发生的概率与事件已经发生次数的概率相等。
调整因子P(A|B)/P(A)<1：说明事件可能发生的概率与事件小于已经发生次数的概率。
因此，贝叶斯推断可以理解为通过先验概率和调整因子来获得后验概率。其中调整因子是根据事件已经发生的概率推断事件可能发生的概率（通过硬币正面出现的次数来推断硬币均匀的可能性），并与已经发生的先验概率（硬币正面出现的概率）的比值。通过这个比值调整先验概率来获得后验概率。
后验概率＝先验概率ｘ调整因子

34. 请你说一说交叉熵，也可以再说一下其他的你了解的熵？
为了更好的理解，需要了解的概率必备知识有：
大写字母X表示随机变量，小写字母x表示随机变量X的某个具体的取值；
P(X)表示随机变量X的概率分布，P(X,Y)表示随机变量X、Y的联合概率分布，P(Y|X)表示已知随机变量X的情况下随机变量Y的条件概率分布；
p(X=x)表示随机变量X取某个具体值的概率，简记为p(x)；
p(X=x,Y=y) 表示联合概率，简记为p(x,y)，p(Y=y|X=x)表示条件概率，简记为p(y|x)，且有：p(x,y)=p(x)*p(y|x)。
- 熵：如果一个随机变量X的可能取值为X={x1,x2,…,xk}，其概率分布为P(X=xi)=pi（i= 1,2,...,n），则随机变量X的熵定义为：

\[H(X) =  - \sum\limits_x {p(x)\log p(x)} \]

联合熵：两个随机变量X，Y的联合分布，可以形成联合熵Joint Entropy，用H(X,Y)表示。

条件熵：在随机变量X发生的前提下，随机变量Y发生所新带来的熵定义为Y的条件熵，用H(Y|X)表示，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。
且有此式子成立：H(Y|X)=H(X,Y)-H(X)，整个式子表示(X,Y)发生所包含的熵减去X单独发生包含的熵。至于怎么得来的请看推导

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/xaingduishang.png" width = "380" height = "280" /></div>
简单解释下上面的推导过程。整个式子共6行，其中
第二行推到第三行的依据是边缘分布p(x)等于联合分布p(x,y)的和；
第三行推到第四行的依据是把公因子logp(x)乘进去，然后把x,y写在一起；
第四行推到第五行的依据是：因为两个sigma都有p(x,y)，故提取公因子p(x,y)放到外边，然后把里边的-logp(x,y)-logp(x)）写成-log(p(x,y)/p(x)) ；
第五行推到第六行的依据是：p(x,y)=p(x)*p(y|x)，故p(x,y)/p(x)=p(y|x)。

- 相对熵：又称互熵，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度等。设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是：
\[D(p\left\| q \right.) = \sum\limits_x {p(x)\log \frac{{p(x)}}{{q(x)}}} {\rm{ = }}{{\rm{E}}_{p(x)}}\log \frac{{p(x)}}{{q(x)}}\]

### 回归问题
35. L1和L2正则化的区别？
L1是模型各个参数的绝对值之和,L2为各个参数平方和的开方值。L1更趋向于产生少量的特征,其它特征为0,最优的参数值很大概率出现在坐标轴上,从而导致产生稀疏的权重矩阵,而L2会选择更多的矩阵,但是这些矩阵趋向于0。

36. Loss Function有哪些，怎么用？
平方损失（预测问题）、交叉熵（分类问题）、hinge损失（SVM支持向量机）、CART回归树的残差损失

37. 线性回归的表达式，损失函数；？
线性回归y=wx+b，w和x可能是多维。线性回归的损失函数为平方损失函数。
解析：一般会要求反向求导推导

38. 线性回归的损失函数？
\[\begin{array}{l}
J(\theta ) = \frac{1}{2}\sum\nolimits_{i = 1}^m {{{(h({x_i}) - {y_i})}^2}} \\
\frac{{\min }}{\theta }J(\theta )
\end{array}\]

39. 知道哪些传统机器学习模型？
常见的机器学习算法：
1）.回归算法：回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。 常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。

2）.基于实例的算法：基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map，SOM）。深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。

3）.决策树学习：决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree，CART），ID3 (Iterative Dichotomiser 3)，C4.5，Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest），多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine，GBM）。

4）.贝叶斯方法：贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators，AODE），以及Bayesian Belief Network（BBN）。

5）.基于核的算法：基于核的算法中最著名的莫过于支持向量机（SVM）了。基于核的算法把输入数据映射到一个高阶的向量空间，在这些高阶向量空间里，有些分类或者回归问题能够更容易的解决。常见的基于核的算法包括：支持向量机（Support Vector Machine，SVM）， 径向基函数（Radial Basis Function，RBF)，以及线性判别分析（Linear Discriminate Analysis，LDA)等。

6）.聚类算法：聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization，EM）。

7）.降低维度算法：像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis，PCA），偏最小二乘回归（Partial Least Square Regression，PLS），Sammon映射，多维尺度（Multi-Dimensional Scaling, MDS）, 投影追踪（Projection Pursuit）等。

8）.关联规则学习：关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori算法和Eclat算法等。

9）.集成算法：集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting，Bootstrapped Aggregation（Bagging），AdaBoost，堆叠泛化（Stacked Generalization，Blending），梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest）。

10）.人工神经网络：人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation），Hopfield网络，自组织映射（Self-Organizing Map, SOM）。学习矢量量化（Learning Vector Quantization， LVQ）。

### 聚类问题
40. 什么是DBSCAN？
DBSCAN是一种基于密度的空间聚类算法,它不需要定义簇的个数,而是将具有足够高密度的区域划分为簇,并在有噪声的数据中发现任意形状的簇,在此算法中将簇定义为密度相连的点的最大集合。

41. k-means算法流程?
从数据集中随机选择k个聚类样本作为初始的聚类中心,然后计算数据集中每个样本到这k个聚类中心的距离,并将此样本分到距离最小的聚类中心所对应的类中。将所有样本归类后,对于每个类别重新计算每个类别的聚类中心即每个类中所有样本的质心,重复以上操作直到聚类中心不变为止。

42. LDA的原理
LDA是一种基于有监督学习的降维方式,将数据集在低维度的空间进行投影,要使得投影后的同类别的数据点间的距离尽可能的靠近,而不同类别间的数据点的距离尽可能的远。

43. 介绍几种机器学习的算法，我就结合我的项目经理介绍了些RF, Kmeans等算法。
常见的机器学习算法：
1）. 回归算法：回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。 常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。

2）. 基于实例的算法：基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map，SOM）。深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。

3）. 决策树学习：决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree，CART），ID3 (Iterative Dichotomiser 3)，C4.5，Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest），多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine，GBM）。

4）. 贝叶斯方法：贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators，AODE），以及Bayesian Belief Network（BBN）。

5）. 基于核的算法：基于核的算法中最著名的莫过于支持向量机（SVM）了。基于核的算法把输入数据映射到一个高阶的向量空间，在这些高阶向量空间里，有些分类或者回归问题能够更容易的解决。常见的基于核的算法包括：支持向量机（Support Vector Machine，SVM）， 径向基函数（Radial Basis Function，RBF)，以及线性判别分析（Linear Discriminate Analysis，LDA)等。

6）. 聚类算法：聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization，EM）。

7）. 降低维度算法：像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis，PCA），偏最小二乘回归（Partial Least Square Regression，PLS），Sammon映射，多维尺度（Multi-Dimensional Scaling, MDS）, 投影追踪（Projection Pursuit）等。

8）. 关联规则学习：关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori算法和Eclat算法等。

9）. 集成算法：集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting，Bootstrapped Aggregation（Bagging），AdaBoost，堆叠泛化（Stacked Generalization，Blending），梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest）。

10）. 人工神经网络：人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation），Hopfield网络，自组织映射（Self-Organizing Map, SOM）。学习矢量量化（Learning Vector Quantization， LVQ）。

RF：通过对训练数据样本以及属性进行有放回的抽样（针对某一个属性随机选择样本）这里有两种，一种是每次都是有放回的采样，有些样本是重复的，组成和原始数据集样本个数一样的数据集；另外一种是不放回的抽样，抽取出大约60%的训练信息。由此生成一颗CART树，剩下的样本信息作为袋外数据，用来当作验证集计算袋外误差测试模型；把抽取出的样本信息再放回到原数据集中，再重新抽取一组训练信息，再以此训练数据集生成一颗CART树。这样依次生成多颗CART树，多颗树组成森林，并且他们的生成都是通过随机采样的训练数据生成，因此叫随机森林。RF可以用于数据的回归，也可以用于数据的分类。回归时是由多颗树的预测结果求均值；分类是由多棵树的预测结果进行投票。正式由于它的随机性，RF有极强的防止过拟合的特性。由于他是由CART组成，因此它的训练数据不需要进行归一化，因为每课的建立过程都是通过选择一个能最好的对数据样本进行选择的属性来建立分叉，因此有以上好处的同时也带来了一个缺点，那就是忽略了属性与属性之间的关系。

K-meas：基本K-Means算法的思想很简单，事先确定常数K，常数K意味着最终的聚类类别数，首先随机选定初始点为质心，并通过计算每一个样本与质心之间的相似度(这里为欧式距离)，将样本点归到最相似的类中，接着，重新计算每个类的质心(即为类中心)，重复这样的过程，知道质心不再改变，最终就确定了每个样本所属的类别以及每个类的质心。由于每次都要计算所有的样本与每一个质心之间的相似度，故在大规模的数据集上，K-Means算法的收敛速度比较慢。

初始化常数K，随机选取初始点为质心

重复计算一下过程，直到质心不再改变

计算样本与每个质心之间的相似度，将样本归类到最相似的类中

重新计算质心

输出最终的质心以及每个类

44. KMeans讲讲，KMeans有什么缺点，K怎么确定?
在k-means算法中，用质心来表示cluster；且容易证明k-means算法收敛等同于所有质心不再发生变化。基本的k-means算法流程如下：
选取k个初始质心（作为初始cluster）；

repeat： 对每个样本点，计算得到距其最近的质心，将其类别标为该质心所对应的cluster； 重新计算k个cluser对应的质心；

until 质心不再发生变化

k-means存在缺点：

1）k-means是局部最优的，容易受到初始质心的影响；比如在下图中，因选择初始质心不恰当而造成次优的聚类结果。

2）同时，k值的选取也会直接影响聚类结果，最优聚类的k值应与样本数据本身的结构信息相吻合，而这种结构信息是很难去掌握，因此选取最优k值是非常困难的。

K值得确定：
法1：(轮廓系数)在实际应用中，由于Kmean一般作为数据预处理，或者用于辅助分聚类贴标签。所以k一般不会设置很大。可以通过枚举，令k从2到一个固定值如10，在每个k值上重复运行数次kmeans(避免局部最优解)，并计算当前k的平均轮廓系数，最后选取轮廓系数最大的值对应的k作为最终的集群数目。

45. Kmeans
基本K-Means算法的思想很简单，事先确定常数K，常数K意味着最终的聚类类别数，首先随机选定初始点为质心，并通过计算每一个样本与质心之间的相似度(这里为欧式距离)，将样本点归到最相似的类中，接着，重新计算每个类的质心(即为类中心)，重复这样的过程，知道质心不再改变，最终就确定了每个样本所属的类别以及每个类的质心。由于每次都要计算所有的样本与每一个质心之间的相似度，故在大规模的数据集上，K-Means算法的收敛速度比较慢。
初始化常数K，随机选取初始点为质心

重复计算一下过程，直到质心不再改变

计算样本与每个质心之间的相似度，将样本归类到最相似的类中

重新计算质心

输出最终的质心以及每个类

46. DBSCAN原理和算法伪代码，与kmeans，OPTICS区别?
DBSCAN聚类算法原理
DBSCAN（Density-Based Spatial Clustering of Applications with Noise）聚类算法，它是一种基于高密度连通区域的、基于密度的聚类算法，能够将具有足够高密度的区域划分为簇，并在具有噪声的数据中发现任意形状的簇。我们总结一下DBSCAN聚类算法原理的基本要点：

DBSCAN算法需要选择一种距离度量，对于待聚类的数据集中，任意两个点之间的距离，反映了点之间的密度，说明了点与点是否能够聚到同一类中。由于DBSCAN算法对高维数据定义密度很困难，所以对于二维空间中的点，可以使用欧几里德距离来进行度量。

DBSCAN算法需要用户输入2个参数：一个参数是半径（Eps），表示以给定点P为中心的圆形邻域的范围；另一个参数是以点P为中心的邻域内最少点的数量（MinPts）。如果满足：以点P为中心、半径为Eps的邻域内的点的个数不少于MinPts，则称点P为核心点。

DBSCAN聚类使用到一个k-距离的概念，k-距离是指：给定数据集P={p(i); i=0,1,…n}，对于任意点P(i)，计算点P(i)到集合D的子集S={p(1), p(2), …, p(i-1), p(i+1), …, p(n)}中所有点之间的距离，距离按照从小到大的顺序排序，假设排序后的距离集合为D={d(1), d(2), …, d(k-1), d(k), d(k+1), …,d(n)}，则d(k)就被称为k-距离。也就是说，k-距离是点p(i)到所有点（除了p(i)点）之间距离第k近的距离。对待聚类集合中每个点p(i)都计算k-距离，最后得到所有点的k-距离集合E={e(1), e(2), …, e(n)}。

根据经验计算半径Eps：根据得到的所有点的k-距离集合E，对集合E进行升序排序后得到k-距离集合E’，需要拟合一条排序后的E’集合中k-距离的变化曲线图，然后绘出曲线，通过观察，将急剧发生变化的位置所对应的k-距离的值，确定为半径Eps的值。

根据经验计算最少点的数量MinPts：确定MinPts的大小，实际上也是确定k-距离中k的值，DBSCAN算法取k=4，则MinPts=4。

另外，如果觉得经验值聚类的结果不满意，可以适当调整Eps和MinPts的值，经过多次迭代计算对比，选择最合适的参数值。可以看出，如果MinPts不变，Eps取得值过大，会导致大多数点都聚到同一个簇中，Eps过小，会导致已一个簇的分裂；如果Eps不变，MinPts的值取得过大，会导致同一个簇中点被标记为噪声点，MinPts过小，会导致发现大量的核心点。

我们需要知道的是，DBSCAN算法，需要输入2个参数，这两个参数的计算都来自经验知识。半径Eps的计算依赖于计算k-距离，DBSCAN取k=4，也就是设置MinPts=4，然后需要根据k-距离曲线，根据经验观察找到合适的半径Eps的值，下面的算法实现过程中，我们会详细说明。对于算法的实现，首先我们概要地描述一下实现的过程：

1）解析样本数据文件。2）计算每个点与其他所有点之间的欧几里德距离。3）计算每个点的k-距离值，并对所有点的k-距离集合进行升序排序，输出的排序后的k-距离值。4）将所有点的k-距离值，在Excel中用散点图显示k-距离变化趋势。5）根据散点图确定半径Eps的值。）根据给定MinPts=4，以及半径Eps的值，计算所有核心点，并建立核心点与到核心点距离小于半径Eps的点的映射。7）根据得到的核心点集合，以及半径Eps的值，计算能够连通的核心点，得到噪声点。8）将能够连通的每一组核心点，以及到核心点距离小于半径Eps的点，都放到一起，形成一个簇。9）选择不同的半径Eps，使用DBSCAN算法聚类得到的一组簇及其噪声点，使用散点图对比聚类效果。

算法伪代码：

算法描述：

算法：DBSCAN

输入：E——半径

MinPts——给定点在E邻域内成为核心对象的最小邻域点数。

D——集合。

输出：目标类簇集合

方法：Repeat

1）判断输入点是否为核心对象

2）找出核心对象的E邻域中的所有直接密度可达点。

Until 所有输入点都判断完毕

Repeat

针对所有核心对象的E邻域内所有直接密度可达点找到最大密度相连对象集合，中间涉及到一些密度可达对象的合并。Until 所有核心对象的E领域都遍历完毕

DBSCAN和Kmeans的区别：

1)K均值和DBSCAN都是将每个对象指派到单个簇的划分聚类算法，但是K均值一般聚类所有对象，而DBSCAN丢弃被它识别为噪声的对象。

2)K均值使用簇的基于原型的概念，而DBSCAN使用基于密度的概念。

3)K均值很难处理非球形的簇和不同大小的簇。DBSCAN可以处理不同大小或形状的簇，并且不太受噪声和离群点的影响。当簇具有很不相同的密度时，两种算法的性能都很差。

4)K均值只能用于具有明确定义的质心（比如均值或中位数）的数据。DBSCAN要求密度定义（基于传统的欧几里得密度概念）对于数据是有意义的。

5)K均值可以用于稀疏的高维数据，如文档数据。DBSCAN通常在这类数据上的性能很差，因为对于高维数据，传统的欧几里得密度定义不能很好处理它们。

6)K均值和DBSCAN的最初版本都是针对欧几里得数据设计的，但是它们都被扩展，以便处理其他类型的数据。

7)基本K均值算法等价于一种统计聚类方法（混合模型），假定所有的簇都来自球形高斯分布，具有不同的均值，但具有相同的协方差矩阵。DBSCAN不对数据的分布做任何假定。

8)K均值DBSCAN和都寻找使用所有属性的簇，即它们都不寻找可能只涉及某个属性子集的簇。

9)K均值可以发现不是明显分离的簇，即便簇有重叠也可以发现，但是DBSCAN会合并有重叠的簇。

10)K均值算法的时间复杂度是O(m)，而DBSCAN的时间复杂度是O(m^2)，除非用于诸如低维欧几里得数据这样的特殊情况。

11)DBSCAN多次运行产生相同的结果，而K均值通常使用随机初始化质心，不会产生相同的结果。

12)DBSCAN自动地确定簇个数，对于K均值，簇个数需要作为参数指定。然而，DBSCAN必须指定另外两个参数：Eps（邻域半径）和MinPts（最少点数）。

13)K均值聚类可以看作优化问题，即最小化每个点到最近质心的误差平方和，并且可以看作一种统计聚类（混合模型）的特例。DBSCAN不基于任何形式化模型。

DBSCAN与OPTICS的区别：

DBSCAN算法，有两个初始参数E（邻域半径）和minPts(E邻域最小点数)需要用户手动设置输入，并且聚类的类簇结果对这两个参数的取值非常敏感，不同的取值将产生不同的聚类结果，其实这也是大多数其他需要初始化参数聚类算法的弊端。

为了克服DBSCAN算法这一缺点，提出了OPTICS算法（Ordering Points to identify the clustering structure）。OPTICS并 不显示的产生结果类簇，而是为聚类分析生成一个增广的簇排序（比如，以可达距离为纵轴，样本点输出次序为横轴的坐标图），这个排序代表了各样本点基于密度 的聚类结构。它包含的信息等价于从一个广泛的参数设置所获得的基于密度的聚类，换句话说，从这个排序中可以得到基于任何参数E和minPts的DBSCAN算法的聚类结果。

OPTICS两个概念：

核心距离：对象p的核心距离是指是p成为核心对象的最小E’。如果p不是核心对象，那么p的核心距离没有任何意义。

可达距离：对象q到对象p的可达距离是指p的核心距离和p与q之间欧几里得距离之间的较大值。如果p不是核心对象，p和q之间的可达距离没有意义。

算法描述：OPTICS算法额外存储了每个对象的核心距离和可达距离。基于OPTICS产生的排序信息来提取类簇。

### 推荐系统算法
47. 请你说一说推荐算法，fm，lr，embedding
推荐算法：
基于人口学的推荐、基于内容的推荐、基于用户的协同过滤推荐、基于项目的协同过滤推荐、基于模型的协同过滤推荐、基于关联规则的推荐
FM：
\[y(x) = {w_0} + \sum\limits_{i = 1}^n {{w_i}{x_i}}  + \sum\limits_{i = 1}^n {\sum\limits_{j = i + 1}^n {({v_i},{v_j}){x_i}{x_j}} } \]

LR：

逻辑回归本质上是线性回归，只是在特征到结果的映射中加入了一层逻辑函数g(z)，即先把特征线性求和，然后使用函数g(z)作为假设函数来预测。g(z)可以将连续值映射到0 和1。
Embedding：

Embedding在数学上表示一个maping：$f:X-->Y$，也就是一个function。其中该函数满足两个性质：1）injective （单射的）：就是我们所说的单射函数，每个Y只有唯一的X对应;2）structure-preserving（结构保存）：比如在X所属的空间上，那么映射后在Y所属空间上同理$y_1<=y_2$。
那么对于word embedding,就是找到一个映射(函数)将单词(word)映射到另外一个空间(其中这个映射具有injective和structure-preserving的特点),生成在一个新的空间上的表达，该表达就是word representation。

48. 协同过滤的itemCF，userCF区别适用场景?
Item CF 和 User CF两个方法都能很好的给出推荐，并可以达到不错的效果。但是他们之间还是有不同之处的，而且适用性也有区别。下面进行一下对比
计算复杂度：

Item CF 和 User CF 是基于协同过滤推荐的两个最基本的算法，User CF 是很早以前就提出来了，Item CF 是从 Amazon 的论文和专利发表之后（2001 年左右）开始流行，大家都觉得 Item CF 从性能和复杂度上比 User CF 更优，其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，同时也不必频繁更新。但我们往往忽略了这种情况只适应于提供商品的电子商务网站，对于新闻，博客或者微内容的推荐系统，情况往往是相反的，物品的数量是海量的，同时也是更新频繁的，所以单从复杂度的角度，这两个算法在不同的系统中各有优势，推荐引擎的设计者需要根据自己应用的特点选择更加合适的算法。

适用场景：

在非社交网络的网站中，内容内在的联系是很重要的推荐原则，它比基于相似用户的推荐原则更加有效。比如在购书网站上，当你看一本书的时候，推荐引擎会给你推荐相关的书籍，这个推荐的重要性远远超过了网站首页对该用户的综合推荐。可以看到，在这种情况下，Item CF 的推荐成为了引导用户浏览的重要手段。同时 Item CF 便于为推荐做出解释，在一个非社交网络的网站中，给某个用户推荐一本书，同时给出的解释是某某和你有相似兴趣的人也看了这本书，这很难让用户信服，因为用户可能根本不认识那个人；但如果解释说是因为这本书和你以前看的某本书相似，用户可能就觉得合理而采纳了此推荐。

相反的，在现今很流行的社交网络站点中，User CF 是一个更不错的选择，User CF 加上社会网络信息，可以增加用户对推荐解释的信服程度。

49. 推荐系统的大概步骤，解决冷启动?
步骤：1）收集用户的所有信息。2）使用大数据计算平台对收集的信息进行处理，的到用户偏好数据。3）将偏好数据导入喜好类型计算算法中进行预算计算，的到预算结果。4）将推荐的结果导入数据库（redis、hbase）。5）发开一个推荐引擎，对外开放接口，输出推荐结果。
解决冷启动的方案：

1）提供非个性化的推荐

最简单的例子就是提供热门排行榜，可以给用户推荐热门排行榜，等到用户数据收集到一定的时候，再切换为个性化推荐。例如Netflix的研究也表明新用户在冷启动阶段确实是更倾向于热门排行榜的，老用户会更加需要长尾推荐

2）利用用户注册信息

用户的注册信息主要分为3种：（1）获取用户的注册信息；（2）根据用户的注册信息对用户分类；（3）给用户推荐他所属分类中用户喜欢的物品。

3）选择合适的物品启动用户的兴趣

用户在登录时对一些物品进行反馈，收集用户对这些物品的兴趣信息，然后给用户推荐那些和这些物品相似的物品。一般来说，能够用来启动用户兴趣的物品需要具有以下特点：

比较热门，如果要让用户对物品进行反馈，前提是用户得知道这是什么东西；

具有代表性和区分性，启动用户兴趣的物品不能是大众化或老少咸宜的，因为这样的物品对用户的兴趣没有区分性；

启动物品集合需要有多样性，在冷启动时，我们不知道用户的兴趣，而用户兴趣的可能性非常多，为了匹配多样的兴趣，我们需要提供具有很高覆盖率的启动物品集合，这些物品能覆盖几乎所有主流的用户兴趣

4）利用物品的内容信息

用来解决物品的冷启动问题，即如何将新加入的物品推荐给对它感兴趣的用户。物品冷启动问题在新闻网站等时效性很强的网站中非常重要，因为这些网站时时刻刻都有新物品加入，而且每个物品必须能够再第一时间展现给用户，否则经过一段时间后，物品的价值就大大降低了。

5）采用专家标注

很多系统在建立的时候，既没有用户的行为数据，也没有充足的物品内容信息来计算物品相似度。这种情况下，很多系统都利用专家进行标注。

6）利用用户在其他地方已经沉淀的数据进行冷启动

以QQ音乐举例：QQ音乐的猜你喜欢电台想要去猜测第一次使用QQ音乐的用户的口味偏好，一大优势是可以利用其它腾讯平台的数据，比如在QQ空间关注了谁，在腾讯微博关注了谁，更进一步，比如在腾讯视频刚刚看了一部动漫，那么如果QQ音乐推荐了这部动漫里的歌曲，用户会觉得很人性化。这就是利用用户在其它平台已有的数据。

再比如今日头条：它是在用户通过新浪微博等社交网站登录之后，获取用户的关注列表，并且爬取用户最近参与互动的feed（转发/评论等），对其进行语义分析，从而获取用户的偏好。

所以这种方法的前提是，引导用户通过社交网络账号登录，这样一方面可以降低注册成本提高转化率；另一方面可以获取用户的社交网络信息，解决冷启动问题。

7）利用用户的手机等兴趣偏好进行冷启动

Android手机开放的比较高，所以在安装自己的app时，就可以顺路了解下手机上还安装了什么其他的app。比如一个用户安装了美丽说、蘑菇街、辣妈帮、大姨妈等应用，就可以判定这是女性了，更进一步还可以判定是备孕还是少女。目前读取用户安装的应用这部分功能除了app应用商店之外，一些新闻类、视频类的应用也在做，对于解决冷启动问题有很好的帮助。

50. 用mapreduce实现10亿级以上数据的kmeans?
算法1.map(key,value)
输入：全局变量centers，偏移量key，样本value

输出：<key’,value>对，其中key’是最近中心的索引，value’是样本信息的字符串

从value构造样本的instance；
```
minDis=Double.MAX_VALUE；
Index=-1；
For i=0 to centers.length do
dis=ComputeDist(instance,centers[i]);
If dis<minDis{
minDis=dis;
index=i;
}
End For
```

把index作为key’；

把不同维度的values构造成value’；

输出<key’,value’>对；

End

注意这里的Step 2和Step 3初始化了辅助变量minDis和index；Step 4通过计算找出了与样本最近的中心点，函数ComputeDist(instance,centers[i])返回样本和中心点centers[i]的距离；Step 8输出了用来进行下一个过程（combiner）的中间数据。

Combine函数. 每个map任务完成之后，我们用combiner去合并同一个map任务的中间结果。因为中间结果是存储在结点的本地磁盘上，所以这个过程不会耗费网络传输的代价。在combine函数中，我们把属于相同簇的values求和。为了计算每个簇的对象的平均值，我们需要记录每个map的每个簇中样本的总数。Combine函数的伪代码见算法2.

算法2.combine(key,V)

输入：key为簇的索引，V为属于该簇的样本列表

输出：<key’,value’>对，key’为簇的索引，value’是由属于同一类的所有样本总和以及样本数所组成的字符串。

初始化一个数组，用来记录同一类的所有样本的每个维度的总和，样本是V中的元素；

初始化一个计数器num为0来记录属于同一类的样本总数；

While(V.hasNext()){

从V.next()构造样本实例instance；

把instance的不同维度值相加到数组

num++;

}

把key作为key’；

构造value’：不同维度的求和结果+num；

输出<key’,value’>对；

End

Reduce函数. Reduce函数的输入数据由每个结点的combine函数获得。如combine函数所描述，输入数据包括部分样本（同一类）的求和以及对应样本数。在reduce函数中，我们可以把同一类的所有样本求和并且计算出对应的样本数。因此，我们可以得到用于下一轮迭代的新中心。Reduce函数的伪代码见算法3。

算法3.Reduce(key,V)

输入：key为簇的索引，V为来自不同结点的部分总和的样本列表

输出：<key’,value’>对，key’为簇的索引，value’是代表新的聚类中心的字符串

初始化一个数组，用来记录同一类的所有样本的每个维度的总和，样本是V中的元素；

初始化一个计数器NUM为0来记录属于同一类的样本总数；

While(V.hasNext()){

从V.next()构造样本实例instance；

把instance的不同维度值相加到数组

NUM+=num;

}

数组的每个元素除以NUM来获得新的中心坐标；

把key作为key’；

构造value’为所有中心坐标的字符串；

输出<key’,value’>对；

End

### 模型融合
51. bagging和boosting的区别？
Bagging是从训练集中进行子抽样组成每个基模型所需要的子训练集,然后对所有基模型预测的结果进行综合操作产生最终的预测结果。
Boosting中基模型按次序进行训练,而基模型的训练集按照某种策略每次都进行一定的转化,最后以一定的方式将基分类器组合成一个强分类器。

Bagging的训练集是在原始集中有放回的选取,而Boosting每轮的训练集不变,只是训练集中的每个样本在分类器中的权重都会发生变化,此权值会根据上一轮的结果进行调整。

Bagging的各个预测函数可以并行生成,Boosting的各预测函数只能顺序生成。

Bagging中整体模型的期望近似于基模型的期望,所以整体模型的偏差相似于基模型的偏差,因此Bagging中的基模型为强模型(强模型拥有低偏差高方差)。

Boosting中的基模型为弱模型,若不是弱模型会导致整体模型的方差很大。
- 
Bagging和Boosting的区别：
1）样本选择上：Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：Bagging：使用均匀取样，每个样例的权重相等。Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3）预测函数：Bagging：所有预测函数的权重相等。Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4）并行计算：Bagging：各个预测函数可以并行生成。Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

52. XGBOOST和GDBT的区别
GDBT在函数空间中利用梯度下降法进行优化而XGB在函数空间中使用了牛顿法进行优化。即GDBT在优化中使用了一阶导数信息,而XGB对损失函数进行了二阶泰勒展开,用到了一阶和二阶倒数信息。XGB在损失函数中加入了正则项(树叶子节点个数,每个叶子节点上输出score的L2模平方和。对于缺失的样本,XGB可以自动学习出它的分裂方向。GDBT的节点分裂方式使用的是gini系数,XGB通过优化推导出分裂前后的增益来选择分裂节点。XGB在处理每个特征列时可以做到并行。

53. GDBT的原理,以及常用的调参参数？
先用一个初始值去学习一棵树,然后在叶子处得到预测值以及预测后的残差,之后的树则基于之前树的残差不断的拟合得到,从而训练出一系列的树作为模型。
n_estimators基学习器的最大迭代次数,learning_rate学习率，max_lead_nodes最大叶子节点数,max_depth树的最大深度,min_samples_leaf叶子节点上最少样本数。

54. stacking和blending的区别?？
Stacking和blending的区别在于数据的划分,blending用不相交的数据训练不同的基模型,并将其输出取加权平均。而stacking是将数据集划分为两个不相交的集合,在第一个集合的数据集中训练多个模型,在第二个数据集中测试这些模型,将预测结果作为输入,将正确的标签作为输出,再训练一个高层的模型

55. AdaBoost和GBDT的区别,AdaBoost和GBDT的区别

AdaBoost通过调整错分的数据点的权重来改进模型,而GBDT是从负梯度的方向去拟合改进模型。
AdaBoost改变了训练数据的权值,即样本的概率分布,减少上一轮被正确分类的样本权值,提高被错误分类的样本权值,而随机森林在训练每棵树的时候,随机挑选部分训练集进行训练。在对新数据进行预测时,AdaBoost中所有树加权投票进行预测,每棵树的权重和错误率有关,而随机森林对所有树的结果按照少数服从多数的原则进行预测。

56. boosting和bagging在不同情况下的选用？
Bagging与Boosting的区别：
1）取样方式（样本权重）：Bagging是均匀选取，样本的权重相等，Boosting根据错误率取样，错误率越大则权重越大。2）训练集的选择：Bagging随机选择训练集，训练集之间相互独立，Boosting的各轮训练集的选择与前面各轮的学习结果有关。3）预测函数：Bagging各个预测函数没有权重，可以并行生成，Boosting有权重，顺序生成。4）Bagging是减少variance，Boosting是减少bias。

Bagging 是 Bootstrap Aggregating的简称，意思就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的 variance. Bagging 比如 Random Forest 这种先天并行的算法都有这个效果。

Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不不断进行行，误差会越来越小，所以模型的 bias 会不不断降低。这种算法无法并行。

57. gbdt推导和适用场景?
...
适用场景：GBDT几乎可用于所有回归问题（线性/非线性），GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。

58. rf和gbdt基分类器区别，里面的决策树分别长啥样，怎么剪枝
GBDT和RF都是集成方法中的经典模型，我们需要弄清楚下面几个问题：1）GBDT是采用boosing方法，RF采用的是baggging方法；2）bias和variance是解释模型泛化性能的，其实还有噪声。
然后，理解GBDT和RF执行原理，其中GBDT中的核心是通过用分类器（如CART、RF）拟合损失函数梯度，而损失函数的定义就决定了在子区域内各个步长，其中就是期望输出与分类器预测输出的查，即bias；而RF的核心就是自采样（样本随机）和属性随机（所有样本中随机选择K个子样本选择最优属性来划分），样本数相同下的不同训练集产生的各个分类器，即数据的扰动导致模型学习性能的变化，即variance。

Gradient boosting Decision Tree(GBDT)

GB算法中最典型的基学习器是决策树，尤其是CART，正如名字的含义，GBDT是GB和DT的结合。要注意的是这里的决策树是回归树，GBDT中的决策树是个弱模型，深度较小一般不会超过5，叶子节点的数量也不会超过10，对于生成的每棵决策树乘上比较小的缩减系数（学习率<0.1），有些GBDT的实现加入了随机抽样（subsample 0.5<=f <=0.8）提高模型的泛化能力。通过交叉验证的方法选择最优的参数。

Random Forest：

bagging （你懂得，原本叫Bootstrap aggregating），bagging 的关键是重复的对经过bootstrapped采样来的观测集子集进行拟合。然后求平均。。。一个bagged tree充分利用近2/3的样本集。。。所以就有了OOB预估(outof bag estimation)

GBDT和随机森林的相同点：

1）都是由多棵树组成；2）最终的结果都是由多棵树一起决定

GBDT和随机森林的不同点：

1）组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成；

2）组成随机森林的树可以并行生成；而GBDT只能是串行生成；

3）对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来；

4）随机森林对异常值不敏感，GBDT对异常值非常敏感；

5）随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成；

6）随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能。

RF决策树：

学习随机森林模型前，一定要先了解决策树模型。树越深，模型越复杂。

决策树模型的优点如下。1）容易理解和解释，树可以被可视化。2）不需要太多的数据预处理工作，即不需要进行数据归一化，创造哑变量等操作。3）隐含地创造了多个联合特征，并能够解决非线性问题。

GBDT决策树：

迭代决策树GBDT（Gradient Boosting Decision Tree）也被称为是MART（Multiple Additive Regression Tree)）或者是GBRT（Gradient Boosting Regression Tree），也是一种基于集成思想的决策树模型，但是它和Random Forest有着本质上的区别。不得不提的是，GBDT是目前竞赛中最为常用的一种机器学习算法，因为它不仅可以适用于多种场景，更难能可贵的是，GBDT有着出众的准确率。

树的剪枝：

（1）前剪枝( Pre-Pruning)

通过提前停止树的构造来对决策树进行剪枝，一旦停止该节点下树的继续构造，该节点就成了叶节点。一般树的前剪枝原则有：a.节点达到完全纯度；b.树的深度达到用户所要的深度；c.节点中样本个数少于用户指定个数；d.不纯度指标下降的最大幅度小于用户指定的幅度。

（2）后剪枝( Post-Pruning)

首先构造完整的决策树，允许决策树过度拟合训练数据，然后对那些置信度不够的结点的子树用叶结点来替代。CART 采用Cost-Complexity Pruning（代价-复杂度剪枝法），代价(cost) ：主要指样本错分率；复杂度(complexity) ：主要指树t的叶节点数，(Breiman…)定义树t的代价复杂度(cost-complexity):信息熵H(X)，信息增益=H(D)-H(Y|X)，信息增益率=gain(x)/H(x)，Gini系数=1-sum（pk^2），基尼系数就是熵在x=1的地方一阶泰勒展开得到f(x)=1-x，所以gini=sum[x(1-x)]=1-sum(x^2)。

59. 随机森林和 GBDT 的区别

1）随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。2）组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。3）组成随机森林的树可以并行生成；而GBDT只能是串行生成。4）对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。5）随机森林对异常值不敏感；GBDT对异常值非常敏感。6）随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。7）随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。
60. xgboost的特征重要性计算

Xgboost根据结构分数的增益情况计算出来选择哪个特征作为分割点,而某个特征的重要性就是它在所有树中出现的次数之和。
61. xgboost的正则项表达式
参考回答：
 T为叶子节点的个数,w为叶子节点的分数
62. xgboost原理，怎么防过拟合
参考回答：
XGBoost是一个树集成模型，它使用的是K（树的总数为K）个树的每棵树对样本的预测值的和作为该样本在XGBoost系统中的预测，定义函数如下：


对于所给的数据集有n个样本，m个特征，定义为：



其中Xi表示第i个样本，yi表示第i个样本的类别标签。CART树的空间为F，如下：



其中q表示每棵树的结构映射每个样本到相应的叶节点的分数，即q表示树的模型，输入一个样本，根据模型将样本映射到叶节点输出预测的分数；Wq(x)表示树q的所有叶节点的分数组成集合；T是树q的叶节点数量。

所以，由（1）式可以看出，XGBoost的预测值为每棵树的预测值之和，即每棵树相应的叶节点的得分之和（Wi的和，Wi表示第i个叶节点的得分）。

我们的目标就是学习这样的K个树模型f(x).。为了学习模型f(x)，我们定义下面的目标函数：



其中，（2）式右边第一项为损失函数项，即训练误差，是一个可微的凸函数（比如用于回归的均方误差和用于分类的Logistic误差函数等），第二项为正则化项，即每棵树的复杂度之和，目的是控制模型的复杂度，防止过拟合。我们的目标是在L(φ)取得最小化时得出对应的模型f(x)。

由于XGBoost模型中的优化参数是模型f(x)，不是一个具体的值，所以不能用传统的优化方法在欧式空间中进行优化，而是采用additive training的方式去学习模型。每一次保留原来的模型不变，加入一个新的函数f到模型中，如下：


预测值在每一次迭代中加入一个新的函数f目的是使目标函数尽量最大地降低。

因为我们的目标是最小化L(φ)时得到模型f(x)，但是L(φ)中并没有参数f(x)，所以，我们将上图中的最后一式代入L(φ)中可得到如下式子：



对于平方误差（用于回归）来说（3）式转换成如下形式：



对于不是平方误差的情况下，一般会采用泰勒展开式来定义一个近似的目标函数，以方便我们的进一步计算。

根据如下的泰勒展开式，移除高阶无穷小项，得：





（3）式等价于下面的式子：



由于我们的目标是求L(φ)最小化时的模型f(x)（也是变量），当移除常数项时模型的最小值变化，但是取最小值的变量不变（比如：y=x^2+C，无论C去何值，x都在0处取最小值）。所以，为了简化计算，我们移除常数项，得到如下的目标函数：



定义 为叶节点j的实例，重写（4）式，将关于树模型的迭代转换为关于树的叶子节点的迭代，得到如下过程：


此时我们的目标是求每棵树的叶节点j的分数Wj，求出Wj后，将每棵树的Wj相加，即可得到最终的预测的分数。而要想得到最优的Wj的值，即最小化我们的目标函数，所以上式对Wj求偏导，并令偏导数为0，算出此时的W*j为：





将W*j代入原式得：



方程（5）可以用作得分(score)函数来测量树结构q的质量。该得分类似于评估决策树的不纯度得分，除了它是针对更广泛的目标函数得出的。

在xgboost调中，一般有两种方式用于控制过拟合：1）直接控制参数的复杂度：包括max_depth min_child_weight gamma；2）add randomness来使得对训练对噪声鲁棒。包括subsample colsample_bytree，或者也可以减小步长 eta，但是需要增加num_round，来平衡步长因子的减小。

63. xgboost，rf，lr优缺点场景。。。
参考回答：
Xgboost：
优缺点：1）在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。2）xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。3）特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。4）按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。5）xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。

适用场景：分类回归问题都可以。

Rf：

优点：1）表现性能好，与其他算法相比有着很大优势。2）随机森林能处理很高维度的数据（也就是很多特征的数据），并且不用做特征选择。3）在训练完之后，随机森林能给出哪些特征比较重要。4）训练速度快，容易做成并行化方法(训练时，树与树之间是相互独立的)。5）在训练过程中，能够检测到feature之间的影响。6）对于不平衡数据集来说，随机森林可以平衡误差。当存在分类不平衡的情况时，随机森林能提供平衡数据集误差的有效方法。7）如果有很大一部分的特征遗失，用RF算法仍然可以维持准确度。8）随机森林算法有很强的抗干扰能力（具体体现在6,7点）。所以当数据存在大量的数据缺失，用RF也是不错的。9）随机森林抗过拟合能力比较强（虽然理论上说随机森林不会产生过拟合现象，但是在现实中噪声是不能忽略的，增加树虽然能够减小过拟合，但没有办法完全消除过拟合，无论怎么增加树都不行，再说树的数目也不可能无限增加的）。10）随机森林能够解决分类与回归两种类型的问题，并在这两方面都有相当好的估计表现。（虽然RF能做回归问题，但通常都用RF来解决分类问题）。11）在创建随机森林时候，对generlization error(泛化误差)使用的是无偏估计模型，泛化能力强。

缺点：1）随机森林在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合。（PS:随机森林已经被证明在某些噪音较大的分类或者回归问题上回过拟合）。2）对于许多统计建模者来说，随机森林给人的感觉就像一个黑盒子，你无法控制模型内部的运行。只能在不同的参数和随机种子之间进行尝试。3）可能有很多相似的决策树，掩盖了真实的结果。4）对于小数据或者低维数据（特征较少的数据），可能不能产生很好的分类。（处理高维数据，处理特征遗失数据，处理不平衡数据是随机森林的长处）。5）执行数据虽然比boosting等快（随机森林属于bagging），但比单只决策树慢多了。

适用场景：数据维度相对低（几十维），同时对准确性有较高要求时。因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。

Lr：

优点：实现简单，广泛的应用于工业问题上；分类时计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

缺点：当特征空间很大时，逻辑回归的性能不是很好；容易欠拟合，一般准确度不太高

不能很好地处理大量多类特征或变量；只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；对于非线性特征，需要进行转换。

适用场景：LR同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。

64. xgboost特征并行化怎么做的
参考回答：
决策树的学习最耗时的一个步骤就是对特征值进行排序,在进行节点分裂时需要计算每个特征的增益,最终选增益大的特征做分裂,各个特征的增益计算就可开启多线程进行。而且可以采用并行化的近似直方图算法进行节点分裂。
65. xgboost和lightgbm的区别和适用场景

（1）xgboost采用的是level-wise的分裂策略，而lightGBM采用了leaf-wise的策略，区别是xgboost对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是xgboost也进行了分裂，带来了务必要的开销。 leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。
（2）lightgbm使用了基于histogram的决策树算法，这一点不同与xgboost中的 exact 算法，histogram算法在内存和计算代价上都有不小优势。1）内存上优势：很明显，直方图算法的内存消耗为(#data* #features * 1Bytes)(因为对特征分桶后只需保存特征离散化之后的值)，而xgboost的exact算法内存消耗为：(2 * #data * #features* 4Bytes)，因为xgboost既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要32位的浮点数来保存。2）计算上的优势，预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为(#data),而直方图算法只需要遍历桶就行了，时间为(#bin)

（3）直方图做差加速，一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。

（4）lightgbm支持直接输入categorical 的feature，在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个category“的gain。类似于one-hot编码。

（5）xgboost在每一层都动态构建直方图，因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导),所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。

其适用场景根据实际项目和两种算法的优点进行选择。

66. HMM隐马尔可夫模型的参数估计方法是？
EM算法
解析：期望最大化（Expectation-Maximum,EM）算法

67. Bootstrap方法是什么？
从一个数据集中有放回的抽取N次，每次抽M个。
解析：Bagging算法基于bootstrap。面试时结合Bagging算法讲述会更好。

68. 问题：如何防止过拟合？

1.早停法；2.l1和l2正则化；3.神经网络的dropout；4.决策树剪枝；5.SVM的松弛变量；6.集成学习
解析：能够达到模型权重减小，模型简单的效果